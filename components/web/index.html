<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Sandwich</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            max-width: 600px;
            margin: 0 auto;
            padding: 2rem 1rem;
            text-align: center;
            background-color: #f4f4f9;
            color: #333;
        }
        h1 { margin-bottom: 2rem; }
        button {
            padding: 0.75rem 1.5rem;
            font-size: 1rem;
            cursor: pointer;
            border: none;
            border-radius: 4px;
            margin: 0 0.5rem;
            transition: background-color 0.2s;
        }
        #startBtn { background-color: #28a745; color: white; }
        #startBtn:disabled { background-color: #95d6a4; cursor: not-allowed; }
        #stopBtn { background-color: #dc3545; color: white; }
        #stopBtn:disabled { background-color: #e9aeb3; cursor: not-allowed; }
        #status {
            margin-top: 2rem;
            font-weight: 500;
            color: #555;
            min-height: 1.5em;
        }
        .log {
            text-align: left;
            background: #fff;
            border: 1px solid #ddd;
            padding: 1rem;
            margin-top: 2rem;
            height: 300px;
            overflow-y: auto;
            border-radius: 4px;
            font-family: monospace;
            font-size: 0.9rem;
        }
    </style>
</head>
<body>
    <h1>ðŸ¥ª Voice Sandwich</h1>
    <div>
        <button id="startBtn">Start Recording</button>
        <button id="stopBtn" disabled>Stop Recording</button>
    </div>
    <div id="status">Ready</div>
    <div class="log" id="log"></div>

    <script>
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const statusDiv = document.getElementById('status');
        const logDiv = document.getElementById('log');
        
        let ws;
        let mediaRecorder;
        let audioContext;
        let nextStartTime = 0;

        function log(msg) {
            const p = document.createElement('div');
            p.textContent = `[${new Date().toLocaleTimeString()}] ${msg}`;
            logDiv.appendChild(p);
            logDiv.scrollTop = logDiv.scrollHeight;
        }

        startBtn.onclick = async () => {
            statusDiv.innerText = 'Connecting...';
            startBtn.disabled = true;
            
            const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
            ws = new WebSocket(`${protocol}//${window.location.host}/ws`);
            ws.binaryType = 'arraybuffer';
            
            ws.onopen = async () => {
                statusDiv.innerText = 'Connected. Recording...';
                log('WebSocket connected');
                stopBtn.disabled = false;

                try {
                    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    log('Microphone access granted');
                    
                    mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm;codecs=opus' });
                    
                    mediaRecorder.ondataavailable = (event) => {
                        if (event.data.size > 0 && ws.readyState === WebSocket.OPEN) {
                            ws.send(event.data);
                        }
                    };

                    mediaRecorder.start(250); // Send chunks every 250ms
                    log('Recording started');
                } catch (err) {
                    console.error(err);
                    log('Error accessing microphone: ' + err.message);
                    statusDiv.innerText = 'Error';
                    resetUI();
                }
            };

            ws.onmessage = async (event) => {
                log('Received message: ' + event.data);
                // We expect audio buffers from the server (PCM 16000Hz 16-bit Mono)
                if (event.data instanceof ArrayBuffer) {
                    // log(`Received audio chunk (${event.data.byteLength} bytes)`);
                    
                    if (!audioContext) {
                        audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    }
                    if (audioContext.state === 'suspended') {
                        await audioContext.resume();
                    }

                    try {
                        // Ensure even byte length for Int16
                        let buffer = event.data;
                        if (buffer.byteLength % 2 !== 0) {
                            console.warn("Received odd byte length audio chunk, trimming last byte");
                            buffer = buffer.slice(0, buffer.byteLength - 1);
                        }
                        
                        const int16Data = new Int16Array(buffer);
                        const float32Data = new Float32Array(int16Data.length);
                        
                        // Convert Int16 to Float32
                        for (let i = 0; i < int16Data.length; i++) {
                            float32Data[i] = int16Data[i] / 32768.0;
                        }

                        // Create buffer with 1 channel, length, and 16000 sample rate
                        const audioBuffer = audioContext.createBuffer(1, float32Data.length, 16000);
                        audioBuffer.getChannelData(0).set(float32Data);

                        const source = audioContext.createBufferSource();
                        source.buffer = audioBuffer;
                        source.connect(audioContext.destination);

                        const now = audioContext.currentTime;
                        // Add a small jitter buffer (50ms) if we are starting fresh or fell behind
                        // This helps prevent "clicks" between chunks if they arrive with slight variance
                        const bufferingDelay = 0.05; 
                        
                        let startTime = nextStartTime;
                        if (startTime < now) {
                            startTime = now + bufferingDelay;
                        }
                        
                        source.start(startTime);
                        nextStartTime = startTime + audioBuffer.duration;
                    } catch (e) {
                        console.error('Error processing audio chunk', e);
                    }
                } else {
                    log('Received message: ' + event.data);
                }
            };

            ws.onclose = () => {
                statusDiv.innerText = 'Disconnected';
                log('WebSocket disconnected');
                resetUI();
            };
            
            ws.onerror = (e) => {
                console.error(e);
                log('WebSocket error');
                statusDiv.innerText = 'Error';
            };
        };

        stopBtn.onclick = () => {
            log('Stopping recording...');
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
                mediaRecorder.stream.getTracks().forEach(track => track.stop());
            }

            // Do NOT close WS here immediately if we want to receive the response.
            // But currently the server closes WS after sending response.
            resetUI();
        };

        function resetUI() {
            startBtn.disabled = false;
            stopBtn.disabled = true;
            // We do NOT reset nextStartTime here because audio might still be queued playing!
            // We do NOT close audioContext because that kills pending audio.
        }
    </script>
</body>
</html>
